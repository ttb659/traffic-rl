# üö¶ Multi-Agent Reinforcement Learning for Traffic Signal Control (SUMO)

## üìå Project Overview

This project implements a **Multi-Agent Reinforcement Learning (MARL)** approach for **urban traffic signal control** using **SUMO**.
Each traffic light is modeled as an autonomous agent, and coordination between intersections is achieved using **MAPPO (Multi-Agent PPO)** with an optional **Light Graph Neural Network (Light-GNN)** for spatial information sharing.

The project is designed as an **academic-grade implementation**, suitable for:

* Master / engineering thesis
* Research-oriented projects
* Portfolio / CV demonstration in Reinforcement Learning and Intelligent Transportation Systems

---

## üéØ Objectives

* Model a **3√ó3 urban road network** in SUMO
* Control **9 traffic lights (1 agent per intersection)**
* Implement **MAPPO from scratch** (no RLlib, no Stable-Baselines)
* Integrate a **Light-GNN** for spatial coordination
* Compare against **classical baselines**:

  * Fixed-time traffic lights
  * SUMO actuated traffic lights
* Perform a **clean ablation study** (with vs without GNN)

---

## üß† Final Architecture

```
SUMO (3√ó3 intersections)
‚îÇ
‚îú‚îÄ‚îÄ Local Observations (per traffic light)
‚îÇ     ‚îú‚îÄ Queue length per direction (N/S/E/W)
‚îÇ     ‚îú‚îÄ Current signal phase
‚îÇ     ‚îî‚îÄ Time since last phase switch
‚îÇ
‚îú‚îÄ‚îÄ Static Road Graph
‚îÇ     ‚îî‚îÄ Adjacency matrix (direct neighbors)
‚îÇ
‚îú‚îÄ‚îÄ Light-GNN (optional, shared parameters)
‚îÇ     ‚îî‚îÄ Spatial state encoding
‚îÇ
‚îî‚îÄ‚îÄ MAPPO (Actor‚ÄìCritic)
      ‚îú‚îÄ Decentralized Actors (shared policy)
      ‚îî‚îÄ Centralized Critic (global value function)
```

**Training paradigm**: Centralized Training & Decentralized Execution (CTDE)

---

## üèóÔ∏è Project Structure

```
traffic-rl/
‚îú‚îÄ‚îÄ agents/              # Actor & Critic networks
‚îú‚îÄ‚îÄ marl/                # MAPPO implementation
‚îú‚îÄ‚îÄ gnn/                 # Light-GNN & IdentityGNN (ablation)
‚îú‚îÄ‚îÄ graph/               # Graph topology & normalization
‚îú‚îÄ‚îÄ env/                 # Multi-agent SUMO environment
‚îú‚îÄ‚îÄ sumo/                # SUMO network, routes & configs
‚îú‚îÄ‚îÄ baselines/           # Fixed-time & actuated baselines
‚îú‚îÄ‚îÄ logs/                # Training logs (CSV)
‚îú‚îÄ‚îÄ train_mappo.py       # Main training script
‚îú‚îÄ‚îÄ analyze_results.py   # Metrics analysis
‚îú‚îÄ‚îÄ plot_comparison.py   # Bar chart comparison
‚îú‚îÄ‚îÄ plot_learning_curves.py
‚îî‚îÄ‚îÄ README.md
```

---

## üö¶ Environment Details

* **Simulator**: SUMO (via TraCI)
* **Network**: 3√ó3 grid (9 intersections)
* **Agents**: 9 traffic lights
* **Action space**:

  * `0`: keep current phase
  * `1`: switch to next phase
* **Reward**:

  * Negative global waiting time (cooperative reward)
  * Normalized by number of agents

---

## ü§ñ Learning Algorithms

### üîπ MAPPO (Multi-Agent PPO)

* Shared actor parameters
* Centralized critic using joint agent embeddings
* Generalized Advantage Estimation (GAE)
* PPO clipped objective
* CPU-friendly implementation

### üîπ Light-GNN (Optional)

* Static graph (Manhattan grid)
* Lightweight message passing (GCN-style)
* Improves spatial coordination between intersections

### üîπ Ablation

* **MAPPO + GNN**
* **MAPPO without GNN** (IdentityGNN)

---

## üìä Baselines

| Method         | Description                       |
| -------------- | --------------------------------- |
| Fixed TLS      | No learning, static signal phases |
| Actuated TLS   | SUMO built-in actuated control    |
| MAPPO (no GNN) | MARL with local observations only |
| MAPPO + GNN    | Full proposed method ‚≠ê            |

---

## üìà Results Summary

Key metric: **Average Queue Length (lower is better)**

* Reinforcement learning approaches significantly outperform classical traffic control strategies.
* Fixed-time control can outperform actuated control in small, dense networks due to reduced oscillations.
* The Light-GNN introduces higher variance but enables richer spatial coordination and better scalability.

> Full results are available in the `logs/` directory and visualized using the provided plotting scripts.

---

## üß™ How to Run

### 1Ô∏è‚É£ Requirements

* Python ‚â• 3.8
* SUMO (with TraCI)
* PyTorch (CPU version)

```bash
pip install torch numpy pandas matplotlib
```

---

### 2Ô∏è‚É£ Training MAPPO + GNN

```bash
python train_mappo.py
```

To disable the GNN (ablation):

```python
USE_GNN = False
```

---

### 3Ô∏è‚É£ Run Baselines

```bash
python run_fixed_baseline.py
python run_actuated_baseline.py
```

---

### 4Ô∏è‚É£ Plot Results

```bash
python analyze_results.py
python plot_comparison.py
python plot_learning_curves.py
```

---

## üìö References

* Yu et al., *The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games*, NeurIPS 2021
* Wei et al., *CoLight: Learning Network-Level Cooperation for Traffic Signal Control*, AAAI
* PressLight, KDD
* Graph-based MARL for Traffic Signal Control, IEEE T-ITS

---

## üéì Key Takeaways

* Demonstrates a **full MARL pipeline** with SUMO
* Clean **from-scratch MAPPO implementation**
* Proper **ablation study and baselines**
* Designed for **academic rigor and reproducibility**

---

## üë§ Author

**TIDO TAMEKENG BOREL**
Project developed for academic and research purposes.

---

## ‚≠ê Final Note

If you find this project useful, feel free to ‚≠ê the repository or use it as a reference for research and learning purposes.

